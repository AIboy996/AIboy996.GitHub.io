---
tags:
- pytorch
include:
- math
---

# åˆ†å¸ƒå¼è®­ç»ƒ

> [torch.distributed](https://docs.pytorch.org/docs/stable/distributed.html)

torchæä¾›äº†ä¸€äº›ç®€å•å¥½ç”¨çš„apiï¼Œå¸®æˆ‘ä»¬å®ç°åˆ†å¸ƒå¼è®­ç»ƒï¼š

- æ•°æ®å¹¶è¡Œ
    - [DP](https://docs.pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)ï¼ˆData-Parallelï¼‰
    - [DDP](https://pytorch.org/docs/stable/notes/ddp.html)ï¼ˆDistributed Data-Parallelï¼‰
- æ¨¡å‹å¹¶è¡Œ
    - [FSDP2](https://pytorch.org/docs/stable/distributed.fsdp.fully_shard.html)ï¼ˆFully Sharded Data-Parallel Trainingï¼‰
    - [TP](https://pytorch.org/docs/stable/distributed.tensor.parallel.html)ï¼ˆTensor Parallelï¼‰
    - [PP](https://pytorch.org/docs/main/distributed.pipelining.html)ï¼ˆPipeline Parallelï¼‰

å¦‚æœå¯¹åˆ†å¸ƒå¼è¿™ä¸ªtopicå¾ˆé™Œç”Ÿï¼Œæˆ‘åœ¨ä¸‹ä¸€èŠ‚æ¢³ç†äº†ä¸€äº›èµ„æ–™ï¼Œå¯ä»¥å¸®ä½ å¿«é€Ÿäº†è§£åˆ†å¸ƒå¼è®­ç»ƒã€‚

## åˆ†å¸ƒå¼è®­ç»ƒï¼Ÿ

åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿä¸»è¦ä¸ºäº†è§£å†³**å•èŠ‚ç‚¹**ï¼ˆsingle-nodeï¼‰çš„ç®—åŠ›å’Œå†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤šä¸ªæœºå™¨ã€å¤šå¼ GPUã€å¤šä¸ªCPUã€å¤šä¸ªå­˜å‚¨è®¾å¤‡ç­‰ï¼ˆæ³›ç§°ä¸ºå¤šä¸ªèŠ‚ç‚¹ï¼Œmultiple nodesï¼‰åŒæ—¶åä½œæ¥**åŠ é€Ÿè®­ç»ƒ**ã€æˆ–è€…å®Œæˆå•èŠ‚ç‚¹æ— æ³•å®Œæˆçš„**è¶…å¤§æ¨¡å‹è®­ç»ƒ**ã€‚

åˆ†å¸ƒå¼è®­ç»ƒæœ‰å¤šç§å®ç°æ–¹å¼ï¼Œåˆ†åˆ«é€‚ç”¨äºä¸åŒçš„åœºæ™¯ï¼š

- ==æ•°æ®å¹¶è¡Œ==ï¼ˆData Parallelism i.e. DPï¼‰
    - é€‚ç”¨äºæ•°æ®é‡å¤§ï¼Œæ¨¡å‹ä¸å¤§ï¼ˆå•æœºå¯åŠ è½½ï¼‰çš„åœºæ™¯
    - ä¸»è¦ç›®çš„æ˜¯åŠ é€Ÿè®­ç»ƒï¼Œ==å¿«è®­ï¼==
- ==æ¨¡å‹å¹¶è¡Œ==ï¼ˆModel Parallelismï¼‰
    - é€‚ç”¨äºæ¨¡å‹éå¸¸å¤§ï¼ˆå•æœºæ— æ³•åŠ è½½ï¼‰çš„åœºæ™¯
    - å¯ä»¥ç”¨æµæ°´çº¿å¹¶è¡Œï¼ŒæŠŠä¸åŒçš„ç®—å­**ä¸²è”**èµ·æ¥
    - ä¹Ÿå¯ä»¥ä½¿ç”¨å¼ é‡å¹¶è¡Œï¼ŒæŠŠå•å±‚å‚æ•°åˆ‡åˆ†åˆ°ä¸åŒè®¾å¤‡ï¼Œç±»ä¼¼**å¹¶è”**
    - ä¸»è¦ç›®çš„æ˜¯å˜ä¸å¯èƒ½ä¸ºå¯èƒ½ï¼Œ==èƒ½è®­ï¼==
- ==æ··åˆå¹¶è¡Œ==
    - æ•°æ®å’Œæ¨¡å‹éƒ½å¹¶è¡Œ~
    - å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ˆå›¾æºï¼š[openmlsys](https://openmlsys.github.io/chapter_distributed_training/methods.html)ï¼Œåé¢è¿˜æœ‰å‡ ä¸ªå›¾ä¹Ÿæ˜¯æ¥è‡ªè¿™é‡Œï¼‰ï¼š
    ![](https://openmlsys.github.io/_images/ch10-hybrid-parallel.png)

### æ¨¡å‹å¹¶è¡Œ

æ¨¡å‹å¹¶è¡Œçš„ä¸€ä¸ªçŸ¥åæ¡ˆä¾‹æ˜¯[AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)ï¼ˆæ˜¯çš„ï¼Œå°±æ˜¯2012å¹´é‚£ä¸ªä¸¾ä¸–é—»åçš„å·ç§¯ç¥ç»ç½‘ç»œï¼‰ï¼ŒAlexç”¨ä¸¤å¼ æ˜¾å­˜3GBçš„GTX 580å®Œæˆäº†62.3Må‚æ•°è§„æ¨¡çš„è®­ç»ƒï¼š

![](assets/2025-08-07-17-28-02.png)

ç„¶è€Œæ¨¡å‹å¹¶è¡Œéœ€è¦æ¯”è¾ƒå›°éš¾çš„å·¥ç¨‹å®ç°ï¼Œæˆ‘æ¥è§¦ä¸å¤šå°±ä¸æ·±å…¥ä»‹ç»äº†ã€‚æˆ‘ä»¬åé¢ä¸»è¦ä»‹ç»æ•°æ®å¹¶è¡Œã€‚

### æ•°æ®å¹¶è¡Œ

æ•°æ®å¹¶è¡Œç®€å•ä¸€äº›ï¼Œæ›´å¸¸è§ä¹Ÿå®¹æ˜“å®ç°ã€‚æˆ‘ä»¬ç®€å•åˆ†æä¸€ä¸‹å®ƒçš„åŸç†ï¼š

é€šå¸¸æˆ‘ä»¬ç”¨éšæœºæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•æ¥è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œï¼Œæµç¨‹å¦‚ä¸‹ï¼š

- ä»å…¨ä½“æ ·æœ¬ä¸­æŠ½å–ä¸€ä¸ªmini-batchï¼š$x_i \sim X,\quad i=1,2,\cdots,b$
- è®¡ç®—æ¨¡å‹åœ¨è¿™ä¸ªmini-batchä¸Šçš„æŸå¤±å’Œæ¢¯åº¦
    - $\hat{y_i} = f_\theta(x_i)$
    - $l = \sum_{i=1}^b \mathcal{L}(\hat{y_i}, y_i)$
    - $g = \frac{\partial l}{\partial \theta}$
- ç„¶åä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°
    - $\theta_{t+1} = \theta_t - \rho g$
- å¦‚æ­¤å¾ªç¯å¾€å¤

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ¢¯åº¦å®é™…ä¸Šå­˜åœ¨ä¸€ä¸ª**åŠ æ€§ç»“æ„**ï¼š

$$
g = \frac{\partial l}{\partial \theta} = \frac{\partial}{\partial \theta} \sum_{i=1}^b \mathcal{L}(\hat{y_i}, y_i)
$$

å› æ­¤æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥æŠŠä¸€ä¸ªmini-batchçš„è®¡ç®—åˆ†é…ç»™ä¸åŒçš„èŠ‚ç‚¹æ¥å®Œæˆï¼Œæœ€åå†åŠ èµ·æ¥å°±å®Œæˆäº†æ•°æ®å¹¶è¡Œï¼š

![](https://openmlsys.github.io/_images/ch10-data-parallel.png)

$$
g = \frac{\partial l}{\partial \theta} = {\color{blue}\sum_{i=1}^k\frac{\partial}{\partial \theta}  \mathcal{L}(\hat{y_i}, y_i)} + {\color{red}\sum_{i=k+1}^b\frac{\partial}{\partial \theta}  \mathcal{L}(\hat{y_i}, y_i)}
$$

> è“è‰²å’Œçº¢è‰²çš„æ¢¯åº¦åˆ†åˆ«ç”±è®¾å¤‡1å’Œè®¾å¤‡2æ¥è®¡ç®—å³å¯

### åˆ†å¸ƒå¼é€šä¿¡

ä½ ä¼šå‘ç°ï¼Œä¸è®ºæ˜¯æ¨¡å‹å¹¶è¡Œè¿˜æ˜¯æ•°æ®å¹¶è¡Œï¼Œä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯ä¸åŒè®¾å¤‡ä¹‹é—´å¦‚ä½•é€šä¿¡ã€‚

å°±ä»¥æ•°æ®å¹¶è¡Œä¸ºä¾‹ï¼Œæˆ‘ä»¬æŠŠä¸€ä¸ªmini-batchä¸­çš„æ•°æ®**å‘é€**ç»™åˆ°äº†ä¸åŒçš„èŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹å®Œæˆäº†è®¡ç®—ä¹‹åéœ€è¦**åŒæ­¥**è®¡ç®—å‡ºçš„æ¢¯åº¦ï¼Œç„¶åå„è‡ªæ›´æ–°æ¨¡å‹ã€‚

ä¸‹é¢è¿™ä¸ªè¡¨å±•ç¤ºäº†`torch.distributed`æ”¯æŒçš„ä¸åŒåˆ†å¸ƒå¼é€šä¿¡åç«¯ä»¥åŠapiï¼š

![](assets/2025-08-07-17-56-40.png)

ä¸¾ä¾‹æ¥è¯´ï¼Œ`torch.distributed.broadcast`å¯ä»¥æŠŠä¸€ä¸ªtensorå¹¿æ’­ç»™é›†ç¾¤ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œè€Œ`torch.distributed.all_reduce`å¯ä»¥åœ¨é›†ç¾¤å†…äº¤æ¢ã€åŒæ­¥ã€èšåˆæŸä¸ªtensorã€‚

ä¸‹å›¾æ˜¯All Reduceçš„ä¸€ä¸ª[ç¤ºæ„å›¾](https://siboehm.com/articles/22/data-parallel-training)ï¼š

![](assets/bad-allreduce.png)

??? deepseek-summary "Ring All Reduceï¼Ÿ"
    **Ring AllReduce** æ˜¯ä¸€ç§é«˜æ•ˆçš„åˆ†å¸ƒå¼è®¡ç®—ç®—æ³•ï¼Œä¸»è¦ç”¨äºåœ¨å¤šä¸ªè®¾å¤‡ï¼ˆå¦‚GPUæˆ–æœåŠ¡å™¨èŠ‚ç‚¹ï¼‰ä¹‹é—´é«˜æ•ˆèšåˆæ•°æ®ï¼ˆå¦‚æ¢¯åº¦æˆ–å‚æ•°ï¼‰ï¼Œå¸¸ç”¨äºæ·±åº¦å­¦ä¹ è®­ç»ƒä¸­çš„åŒæ­¥å¹¶è¡Œï¼ˆå¦‚æ•°æ®å¹¶è¡Œï¼‰ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡**ç¯å½¢æ‹“æ‰‘ç»“æ„**å’Œ**åˆ†é˜¶æ®µé€šä¿¡**æ¥ä¼˜åŒ–ä¼ ç»ŸAllReduceçš„å¸¦å®½å’Œå»¶è¿Ÿé—®é¢˜ã€‚

    **æ ¸å¿ƒåŸç†**
    
    1. **ç¯å½¢æ‹“æ‰‘**ï¼š
        - æ‰€æœ‰èŠ‚ç‚¹æ’åˆ—æˆä¸€ä¸ªé€»è¾‘ç¯ï¼ˆå¦‚`Node 0 â†’ Node 1 â†’ ... â†’ Node N â†’ Node 0`ï¼‰ã€‚
        - æ¯ä¸ªèŠ‚ç‚¹åªä¸ç›¸é‚»çš„ä¸¤ä¸ªèŠ‚ç‚¹é€šä¿¡ï¼ˆå‰é©±å’Œåç»§ï¼‰ï¼Œé¿å…å…¨è¿æ¥çš„é«˜å¸¦å®½å‹åŠ›ã€‚

    2. **åˆ†é˜¶æ®µæ“ä½œ**ï¼š
        - **Reduce-Scatteré˜¶æ®µ**ï¼š
            - æ•°æ®è¢«åˆ†å—ï¼ˆå¦‚åˆ†æˆNå—ï¼ŒNä¸ºèŠ‚ç‚¹æ•°ï¼‰ã€‚
            - èŠ‚ç‚¹ä¾æ¬¡ä¼ é€’å¹¶ç´¯åŠ ï¼ˆReduceï¼‰æ•°æ®å—ï¼Œç»è¿‡N-1æ­¥åï¼Œæ¯ä¸ªèŠ‚ç‚¹æŒæœ‰æœ€ç»ˆèšåˆç»“æœçš„ä¸€ä¸ªåˆ†å—ã€‚
        - **All-Gatheré˜¶æ®µ**ï¼š
            - èŠ‚ç‚¹äº¤æ¢å·²èšåˆçš„åˆ†å—ï¼Œç»è¿‡N-1æ­¥åï¼Œæ‰€æœ‰èŠ‚ç‚¹è·å¾—å®Œæ•´çš„å…¨å±€èšåˆç»“æœã€‚

    **ä¼˜åŠ¿**
    
    1. **å¸¦å®½ä¼˜åŒ–**ï¼š
        - ä¼ ç»ŸAllReduceéœ€è¦æ¯ä¸ªèŠ‚ç‚¹å‘é€å¤§é‡æ•°æ®ï¼Œè€ŒRing AllReduceå°†é€šä¿¡é‡å‡åŒ€åˆ†å¸ƒåˆ°ç¯ä¸Šï¼Œæ€»é€šä¿¡é‡å›ºå®šä¸º $2(N-1)\times\frac{D}{N}$ï¼ˆDä¸ºæ•°æ®å¤§å°ï¼‰ã€‚
    2. **é¿å…ç“¶é¢ˆ**ï¼š
        - æ— ä¸­å¿ƒèŠ‚ç‚¹ï¼Œæ‰€æœ‰èŠ‚ç‚¹å¹³ç­‰å‚ä¸ï¼Œé¿å…å‚æ•°æœåŠ¡å™¨ï¼ˆPSï¼‰æ¶æ„çš„å¸¦å®½ç“¶é¢ˆã€‚
    3. **é€‚åˆå¤§è§„æ¨¡è®­ç»ƒ**ï¼š
        - æ‰©å±•æ€§å¥½ï¼ŒèŠ‚ç‚¹å¢åŠ æ—¶é€šä¿¡é‡ä»…çº¿æ€§å¢é•¿ï¼ˆè€ŒPSæ¶æ„æ˜¯`O(N)`ï¼‰ã€‚

    **åº”ç”¨åœºæ™¯**
    
    - **æ·±åº¦å­¦ä¹ æ¡†æ¶**ï¼š
        - TensorFlowï¼ˆé€šè¿‡Horovodï¼‰ã€PyTorchï¼ˆ`torch.distributed`ï¼‰ç­‰å‡æ”¯æŒRing AllReduceã€‚
        - ä¾‹å¦‚ï¼Œåœ¨æ•°æ®å¹¶è¡Œä¸­ï¼Œæ¯ä¸ªGPUè®¡ç®—æœ¬åœ°æ¢¯åº¦åï¼Œé€šè¿‡Ring AllReduceåŒæ­¥å…¨å±€æ¢¯åº¦ã€‚
    - **é«˜æ€§èƒ½è®¡ç®—**ï¼š
        - MPIåº“ï¼ˆå¦‚NCCLã€OpenMPIï¼‰å®ç°äº†è¯¥ç®—æ³•ï¼Œç”¨äºGPUé›†ç¾¤çš„é«˜æ•ˆé€šä¿¡ã€‚

    
    **ç¤ºä¾‹ï¼ˆ4ä¸ªèŠ‚ç‚¹ï¼‰**
    
    1. **Reduce-Scatter**ï¼š
        - æ¯ä¸ªèŠ‚ç‚¹å°†æ•°æ®åˆ†4å—ï¼ˆA1,A2,A3,A4ï¼‰ã€‚
        - ç¬¬1æ­¥ï¼šNode 0å‘é€A1ç»™Node 1ï¼ŒNode 1ç´¯åŠ ï¼›åŒæ—¶Node 1å‘é€A2ç»™Node 2ï¼Œä¾æ­¤ç±»æ¨ã€‚
        - ç»è¿‡3æ­¥åï¼ŒNode 0æŒæœ‰æœ€ç»ˆçš„A1å—ï¼ˆA1+B1+C1+D1ï¼‰ï¼Œå…¶ä»–èŠ‚ç‚¹åŒç†ã€‚
    2. **All-Gather**ï¼š
        - èŠ‚ç‚¹äº¤æ¢èšåˆåçš„åˆ†å—ï¼Œæœ€ç»ˆæ‰€æœ‰èŠ‚ç‚¹è·å¾—å®Œæ•´ç»“æœã€‚

    **å¯¹æ¯”å…¶ä»–æ–¹æ³•**
    
    | æ–¹æ³•               | é€šä¿¡å¤æ‚åº¦       | ç“¶é¢ˆé£é™©       | é€‚ç”¨è§„æ¨¡       |
    |--------------------|------------------|----------------|----------------|
    | **Ring AllReduce** | `O(N)`ï¼ˆçº¿æ€§ï¼‰     | æ—              | å¤§è§„æ¨¡é›†ç¾¤     |
    | **å‚æ•°æœåŠ¡å™¨**     | `O(N)`ï¼ˆä¸­å¿ƒèŠ‚ç‚¹ï¼‰ | ä¸­å¿ƒèŠ‚ç‚¹å¸¦å®½   | å°è§„æ¨¡         |
    | **Tree AllReduce** | `O(logN)`          | æ ¹èŠ‚ç‚¹å¸¦å®½     | ä¸­ç­‰è§„æ¨¡       |

    **å±€é™**
    
    - **å»¶è¿Ÿæ•æ„Ÿ**ï¼šç¯å½¢é€šä¿¡çš„æ­¥æ•°éšèŠ‚ç‚¹æ•°å¢åŠ ï¼Œå°è§„æ¨¡æ—¶å¯èƒ½ä¸å¦‚Tree AllReduceå¿«ã€‚
    - **å®¹é”™æ€§å·®**ï¼šä»»ä¸€èŠ‚ç‚¹æ•…éšœä¼šå¯¼è‡´ç¯æ–­è£‚ï¼Œéœ€é¢å¤–æœºåˆ¶å¤„ç†ã€‚

    æ€»ç»“æ¥è¯´ï¼ŒRing AllReduceé€šè¿‡å·§å¦™çš„ç¯å½¢åˆ†å—é€šä¿¡ï¼Œæˆä¸ºåˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ ä¸­çš„ä¸»æµåŒæ­¥ç®—æ³•ï¼Œå°¤å…¶åœ¨GPUé›†ç¾¤è®­ç»ƒä¸­è¡¨ç°ä¼˜å¼‚ã€‚

## DP

> [torch.nn.DataParallel](https://docs.pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)

å‰é¢è®²äº†è¿™ä¹ˆå¤šç†è®ºï¼Œä¸‹é¢æˆ‘ä»¬æ¥å®æˆ˜ä¸€ä¸‹ã€‚torchä¸­å®ç°æ•°æ®å¹¶è¡Œä¸»è¦æœ‰DPå’ŒDDPä¸¤ç§æ–¹æ³•ã€‚

### DPå’ŒDDPçš„å¼‚åŒ

æŒ‰ç…§torchçš„[best practices](https://docs.pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead)çš„è¯´æ³•ï¼Œä¸è®ºæ˜¯å•æœºå¤šå¡è¿˜æ˜¯å¤šä¸ªæœºå™¨ï¼Œæˆ‘ä»¬æ°¸è¿œä¸åº”è¯¥ä½¿ç”¨DPï¼Œè€Œåº”è¯¥ä½¿ç”¨DDPã€‚

??? warning "Why DDP beats DP?"
    DPå’ŒDDPéƒ½æ˜¯åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œçš„å®ç°ã€‚å®ƒä»¬çš„æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼ŒDPä½¿ç”¨Pythonçš„[threading](https://docs.python.org/3/library/threading.html)æ ‡å‡†åº“å®ç°å¹¶è¡Œã€‚è€ŒDDPä½¿ç”¨Pythonçš„[multiprocessing](https://docs.python.org/3/library/multiprocessing.html)æ ‡å‡†åº“å®ç°å¹¶è¡Œã€‚DDPé€šè¿‡ä½¿ç”¨å¤šè¿›ç¨‹ï¼Œæ¯ä¸ªGPUéƒ½æœ‰å…¶ä¸“ç”¨è¿›ç¨‹ï¼Œè¿™é¿å…äº†Pythonè§£é‡Šå™¨çš„GILå¸¦æ¥çš„æ€§èƒ½å¼€é”€ã€‚

    å› æ­¤DPå’ŒDDPçš„åŒºåˆ«å°±æ˜¯**å¤šçº¿ç¨‹**å’Œ**å¤šè¿›ç¨‹**çš„åŒºåˆ«ã€‚

    ??? deepseek-summary "å¤šçº¿ç¨‹ä¸å¤šè¿›ç¨‹çš„åŒºåˆ«ï¼Ÿ"
        å¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹éƒ½æ˜¯å®ç°å¹¶å‘ç¼–ç¨‹çš„æ–¹å¼ï¼Œä½†å®ƒä»¬åœ¨èµ„æºç®¡ç†ã€é€šä¿¡æ–¹å¼å’Œåº”ç”¨åœºæ™¯ä¸Šæœ‰æ˜¾è‘—å·®å¼‚ã€‚

        | ç‰¹æ€§        | å¤šè¿›ç¨‹                          | å¤šçº¿ç¨‹                          |
        |-----------|-------------------------------|-------------------------------|
        | å†…å­˜ç©ºé—´    | æ¯ä¸ªè¿›ç¨‹æœ‰ç‹¬ç«‹çš„å†…å­˜ç©ºé—´               | åŒä¸€è¿›ç¨‹å†…çš„çº¿ç¨‹å…±äº«å†…å­˜ç©ºé—´            |
        | åˆ›å»ºå¼€é”€    | è¾ƒå¤§ï¼ˆéœ€è¦å¤åˆ¶çˆ¶è¿›ç¨‹èµ„æºï¼‰              | è¾ƒå°ï¼ˆå…±äº«è¿›ç¨‹èµ„æºï¼‰                 |
        | é€šä¿¡æ–¹å¼    | è¿›ç¨‹é—´é€šä¿¡(IPC)å¦‚ç®¡é“ã€æ¶ˆæ¯é˜Ÿåˆ—ã€å…±äº«å†…å­˜ç­‰ | å¯ç›´æ¥é€šè¿‡å…±äº«å˜é‡é€šä¿¡                |
        | ç¨³å®šæ€§     | ä¸€ä¸ªè¿›ç¨‹å´©æºƒä¸ä¼šå½±å“å…¶ä»–è¿›ç¨‹             | ä¸€ä¸ªçº¿ç¨‹å´©æºƒå¯èƒ½å¯¼è‡´æ•´ä¸ªè¿›ç¨‹å´©æºƒ          |
        | ä¸Šä¸‹æ–‡åˆ‡æ¢  | å¼€é”€è¾ƒå¤§                         | å¼€é”€è¾ƒå°                        |
        | å¹¶è¡Œæ€§     | å¯çœŸæ­£å¹¶è¡Œï¼ˆåœ¨å¤šæ ¸CPUä¸Šï¼‰             | å—GILé™åˆ¶ï¼ˆå¦‚Pythonï¼‰            |
        | åŒæ­¥éœ€æ±‚    | é€šå¸¸ä¸éœ€è¦åŒæ­¥                     | éœ€è¦åŒæ­¥æœºåˆ¶ï¼ˆå¦‚é”ï¼‰æ¥ä¿æŠ¤å…±äº«æ•°æ®        |
    ??? question "Pythonçš„å¤šçº¿ç¨‹å’ŒGIL"
        å¤šçº¿ç¨‹çœ‹èµ·æ¥éå¸¸å®Œç¾ï¼Œç„¶è€Œæˆ‘ä»¬çŸ¥é“ï¼ŒPythonçš„å¤šçº¿ç¨‹æ˜¯å‡çš„ã€‚ç”±äºå…¨å±€è§£é‡Šå™¨é”ï¼ˆGIL, Global Interpreter Lockï¼‰çš„å­˜åœ¨ï¼ŒPythonçš„å¤šçº¿ç¨‹æ— æ³•çœŸæ­£å¹¶è¡Œæ‰§è¡ŒPythonä»£ç ã€‚å› æ­¤threadingè¿™ä¸ªåº“å®é™…ä¸Šåªé€‚åˆIOå¯†é›†ä»»åŠ¡ï¼Œå¹¶ä¸é€‚åˆè®¡ç®—å¯†é›†ä»»åŠ¡ã€‚

        > ä¸è¿‡æœªæ¥Pythonçš„GILä¼¼ä¹è¦ç§»é™¤ï¼Œè¯´ä¸å®šä»¥åå¤šçº¿ç¨‹å°±æ˜¯æ›´å¥½çš„äº†ã€‚

### ä¸€è¡Œä»£ç å®ç°DP

è™½ç„¶ä¸è¢«æ¨èï¼Œä½†DPæœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼š**ä¸€è¡Œä»£ç å°±å¯ä»¥åœ¨å•æœºå¤šå¡ä¸Šå¼•å…¥**ï¼Œéå¸¸ç®€å•ã€‚æ‰€ä»¥æˆ‘ä»¬è¿˜æ˜¯ä»‹ç»ä¸€ä¸‹å®ƒçš„ç”¨æ³•ã€‚

```python hl_lines="1" title="DPçš„å®ç°æ–¹æ³•"
net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])
output = net(input_var)  # input_var can be on any device, including CPU
```

æ˜¯çš„ï¼Œåªéœ€è¦ç”¨`torch.nn.DataParallel`åŒ…è£…ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å®ç°äº†æ•°æ®å¹¶è¡Œã€‚åŒ…è£…å®Œæ¯•ä¹‹åï¼Œåç»­torchä¼šè‡ªåŠ¨æŠŠè¾“å…¥çš„tensoråˆ‡åˆ†ã€å‘é€åˆ°ä¸åŒçš„deviceè¿›è¡Œè®¡ç®—ã€‚

æ˜¾ç„¶DPåªé€‚ç”¨äºå•æœºå¤šå¡ï¼Œæ— æ³•èƒœä»»å¤šæœºå¤šå¡çš„æƒ…å†µã€‚å¹¶ä¸”ç”±äºGILçš„é—®é¢˜ï¼ŒDPçš„æ€§èƒ½æœ‰æŸè€—ã€‚

## DDP

> [torch.nn.parallel.DistributedDataParallel](https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)

ç›¸æ¯”ä¹‹ä¸‹DDPå°±è¦å¤æ‚åœ°å¤šã€‚

!!! question "DDPçš„å‡ å¥—å®ç°æ–¹å¼"
    æ›´å¤æ‚çš„æ˜¯ï¼Œç”±äºå†å²é—ç•™é—®é¢˜ï¼ŒDDPæœ‰å‡ å¥—å®Œå…¨ä¸åŒçš„å†™æ³•å’Œå¯¹åº”çš„å¯åŠ¨æ–¹å¼ï¼š

    ??? note "ç¬¬ä¸€å¥—ï¼šæ‰‹åŠ¨spawn"
        ç¬¬ä¸€å¥—æ˜¯æœ€åŸå§‹çš„ï¼Œæˆ‘ä»¬è‡ªå·±æ¥åˆ›å»ºå¤šä¸ªè¿›ç¨‹ï¼š
        ```python title="run.py" hl_lines="28-31"
        #!/usr/bin/env python
        import os
        import sys
        import torch
        import torch.distributed as dist
        import torch.multiprocessing as mp

        def run(rank, size):
            """ Distributed function to be implemented later. """
            pass

        def init_process(rank, size, fn, backend='gloo'):
            """ Initialize the distributed environment. """
            os.environ['MASTER_ADDR'] = '127.0.0.1'
            os.environ['MASTER_PORT'] = '29500'
            dist.init_process_group(backend, rank=rank, world_size=size)
            fn(rank, size)


        if __name__ == "__main__":
            world_size = 2
            processes = []
            if "google.colab" in sys.modules:
                print("Running in Google Colab")
                mp.get_context("spawn")
            else:
                mp.set_start_method("spawn")
            for rank in range(world_size):
                p = mp.Process(target=init_process, args=(rank, world_size, run))
                p.start()
                processes.append(p)

            for p in processes:
                p.join()
        ```

        ç„¶åç›´æ¥ç”¨Pythonå¯åŠ¨å°±è¡Œï¼š
        ```bash
        python run.py
        ```
    ??? note "ç¬¬äºŒå¥—ï¼štorch.distributed.launch"
        ç¬¬äºŒå¥—ç¨å¾®ç®€å•ä¸€äº›ï¼Œæˆ‘ä»¬ä½¿ç”¨`torch.distributed.launch`æ¥å¯åŠ¨ï¼š

        ```python title="launch.py" hl_lines="15-19"
        import torch
        import torch.distributed as dist
        import argparse
        import os

        def worker(rank, world_size):
            dist.init_process_group("gloo", rank=rank, world_size=world_size)
            tensor = torch.tensor(rank + 1, dtype=torch.float)
            dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
            if rank == 0:
                print(f"Sum of ranks: {tensor.item()}")
            dist.destroy_process_group()

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--local-rank", type=int)
            args = parser.parse_args()
            world_size = int(os.environ['WORLD_SIZE'])
            worker(args.local_rank, world_size)

        if __name__ == "__main__":
            main()
        ```
        å¯åŠ¨çš„æ–¹å¼æ˜¯ï¼š
        ```bash
        python -m torch.distributed.launch --nproc-per-node=4 launch.py
        ```

        è¿™ç§æ–¹å¼éœ€è¦ä½¿ç”¨argparseå’Œos.environæ¥ä¼ é€’å‚æ•°ï¼Œä¹Ÿå¾ˆéº»çƒ¦ã€‚ä¸è¿‡ä¸éœ€è¦æˆ‘ä»¬æ‰‹åŠ¨spawnå„ä¸ªè¿›ç¨‹ï¼Œå·²ç»ç®€åŒ–éå¸¸å¤šäº†ã€‚
    
    ç¬¬ä¸‰å¥—å°±æ˜¯æˆ‘ä»¬åé¢ä»‹ç»çš„`torchrun`æ–¹å¼ï¼ˆä¹Ÿå«[Elastic Launch](https://docs.pytorch.org/docs/stable/elastic/run.html)ï¼Œæ˜¯`torch.distributed.run`æ¨¡å—çš„è„šæœ¬å°è£…ï¼‰ï¼Œæœ€ç®€å•ä¹Ÿæœ€æ¨èã€‚

### æç®€ä¾‹å­

ä¸‹é¢æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥å±•ç¤ºDDPçš„å®ç°æ–¹æ³•ã€‚ä»£ç æ¥è‡ª[pytorch-ddp-examples](https://github.com/CSCfi/pytorch-ddp-examples/blob/master/mnist_ddp.py)ï¼Œç•¥æœ‰ä¿®æ”¹ï¼š

```python title="DDPçš„å®ç°æ–¹æ³•ï¼šmnist_ddp.py" hl_lines="43 56 62 78"
# Based on multiprocessing example from
# https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html

import os
import argparse
import torch
import torch.nn as nn
import torch.distributed as dist
import torchvision.transforms as transforms

from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel


class ConvNet(nn.Module):
    def __init__(self, num_classes=10):
        super(ConvNet, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.fc = nn.Linear(7 * 7 * 32, num_classes)

    def forward(self, x):
        out = self.cnn(x)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out


def train(num_epochs):
    torch.manual_seed(0)

    # 1ã€åˆå§‹åŒ–é›†ç¾¤
    dist.init_process_group(backend="nccl")
    rank = dist.get_rank()

    # æˆ‘ä»¬åªéœ€è¦åœ¨rank==0çš„è¿›ç¨‹ä¸Šæ‰“å°æ¶ˆæ¯å°±è¡Œäº†
    # å› ä¸ºæ‰€æœ‰è¿›ç¨‹çš„ä¿¡æ¯åº”è¯¥æ˜¯ä¸€è‡´çš„
    verbose = rank == 0
    if verbose:
        print(os.environ)

    # 2ã€å‡†å¤‡æ¨¡å‹
    local_rank = int(os.environ["LOCAL_RANK"])  # è¿›ç¨‹çš„æœ¬æœºrank
    torch.cuda.set_device(local_rank)
    model = ConvNet().cuda()
    model = DistributedDataParallel(model, device_ids=[local_rank])

    # 3ã€å‡†å¤‡æ•°æ®é›†
    train_dataset = MNIST(
        root="./data", train=True, transform=transforms.ToTensor(), download=False
    )
    train_sampler = DistributedSampler(train_dataset)
    batch_size = 100
    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True,
        sampler=train_sampler,
    )

    # ä¸‹é¢çš„è¿‡ç¨‹åŸºæœ¬å’Œæ™®é€šè®­ç»ƒæ— å¼‚
    criterion = nn.CrossEntropyLoss().cuda()
    optimizer = torch.optim.SGD(model.parameters(), 1e-4)
    for epoch in range(num_epochs):
        # éœ€è¦æ‰‹åŠ¨è®¾ç½®ï¼Œå¦åˆ™é‡‡æ ·é¡ºåºä¸€æˆä¸å˜
        train_sampler.set_epoch(epoch)
        tot_loss = 0
        for i, (images, labels) in enumerate(train_loader):
            images = images.cuda(non_blocking=True)
            labels = labels.cuda(non_blocking=True)
            outputs = model(images)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            tot_loss += loss.item()
        if verbose:
            print(
                f"Epoch [{epoch + 1}/{num_epochs}], "
                "average loss: {tot_loss / (i + 1):.4f}"
            )


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--epochs",
        default=10,
        type=int,
        metavar="N",
        help="number of total epochs to run",
    )
    args = parser.parse_args()
    train(args.epochs)


if __name__ == "__main__":
    main()
```

ç„¶åï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨[torchrun](https://docs.pytorch.org/docs/stable/elastic/quickstart.html)æ¥å¯åŠ¨è®­ç»ƒï¼ˆä¸‹é¢çš„ä¾‹å­æ˜¯2ä¸ªèŠ‚ç‚¹ï¼Œæ¯å°èŠ‚ç‚¹8ä¸ªè¿›ç¨‹ï¼‰ï¼š

```bash
torchrun --nnodes=2 --nproc_per_node=8 \
    --rdzv_id=100 --rdzv_backend=c10d \
    --rdzv_endpoint=$MASTER_ADDR:29400 mnist_ddp.py --epochs 10
```

??? question "rdzvæ˜¯ä»€ä¹ˆ"
    rdzvæ˜¯[Rendezvous](https://docs.pytorch.org/docs/stable/elastic/rendezvous.html)çš„ç¼©å†™ï¼Œåœ¨å¤šæœºå™¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­æˆ‘ä»¬éœ€è¦æŒ‡å®šrdzv_endpointï¼ˆä¹Ÿå°±æ˜¯æ±‡èšç‚¹ï¼Œä½œä¸ºmasterå‘å·æ–½ä»¤ï¼‰ã€‚

    ä¸è¿‡è®²çœŸæˆ‘ä¹Ÿæ²¡ç”¨è¿‡ï¼Œç©·é€¼å®éªŒå®¤åªæœ‰å•æœºå¤šå¡å¯ä»¥ç”¨ğŸ˜…ğŸ˜„

    å¦‚æœæ‚¨æœ‰ç›¸å…³çš„èµ„æºï¼Œåº”è¯¥å°±æœ‰äººæä¾›å¯¹åº”çš„æ–‡æ¡£å’Œæ”¯æŒ~

å¦‚æœæ˜¯**å•æœºå¤šå¡**è®­ç»ƒå°±ç®€å•å¤šäº†ï¼Œæœ¬åœ°é€šä¿¡ä¸éœ€è¦rdzvå‚æ•°ï¼Œåªéœ€è¦æŒ‡å®šä½¿ç”¨çš„è¿›ç¨‹æ•°é‡å³å¯ï¼š

```bash
CUDA_VISIBLE_DEVICES="0,1,2,3" torchrun --nproc_per_node=4 mnist_ddp.py --epochs 10
```

ä¸‹é¢æ˜¯è¯¥è„šæœ¬åœ¨æˆ‘çš„ç¯å¢ƒä¸­çš„è¿è¡Œæƒ…å†µï¼š

<figure markdown>
![](assets/2025-08-07-23-16-29.png)
<figurecaption>ç©·é¬¼ä¸“ç”¨V100</figurecaption>
</figure>

æ³¨æ„ï¼Œæˆ‘è¾“å‡ºäº†`rank==0`è¿›ç¨‹çš„ç¯å¢ƒå˜é‡ï¼Œå¯ä»¥çœ‹åˆ°torchrunæ·»åŠ äº†éå¸¸å¤šæœ‰ç”¨çš„ç¯å¢ƒå˜é‡ï¼ˆè¯¦æƒ…é—´[torch/distributed/run.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/run.py)ï¼‰ã€‚ä¾‹å¦‚`LOCAL_RANK`ï¼Œ`LOCAL_WORLD_SIZE`ï¼Œ`WORLD_SIZE`ç­‰ã€‚å¦‚æœ‰å¿…è¦ä½ å¯ä»¥è¯»å–è¿™äº›ç¯å¢ƒå˜é‡ã€‚

```text title="è¿è¡Œæ—¥å¿—" hl_lines="4-21"
environ({
    'SHELL': '/bin/bash', 
    ... # æ­¤å¤„çœç•¥äº†æ— å…³çš„å†…å­˜
    'OMP_NUM_THREADS': '1', 
    'LOCAL_RANK': '0', 
    'RANK': '0', 
    'GROUP_RANK': '0', 
    'ROLE_RANK': '0', 
    'ROLE_NAME': 'default', 
    'LOCAL_WORLD_SIZE': '4', 
    'WORLD_SIZE': '4', 
    'GROUP_WORLD_SIZE': '1', 
    'ROLE_WORLD_SIZE': '4', 
    'MASTER_ADDR': '127.0.0.1', 
    'MASTER_PORT': '29500', 
    'TORCHELASTIC_RESTART_COUNT': '0', 
    'TORCHELASTIC_MAX_RESTARTS': '0', 
    'TORCHELASTIC_RUN_ID': 'none', 
    'TORCHELASTIC_USE_AGENT_STORE': 'True', 
    'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 
    'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic_gfcioxya/none_6ffbx9v6/attempt_0/0/error.json'
    })
Epoch [1/10], average loss: 2.2009
Epoch [2/10], average loss: 1.9731
Epoch [3/10], average loss: 1.7889
Epoch [4/10], average loss: 1.6223
Epoch [5/10], average loss: 1.4896
Epoch [6/10], average loss: 1.3651
Epoch [7/10], average loss: 1.2676
Epoch [8/10], average loss: 1.1728
Epoch [9/10], average loss: 1.1013
Epoch [10/10], average loss: 1.0307

```

ä¸‹é¢æˆ‘ä»¬è¯¦ç»†ä»‹ç»å‡ ä¸ªå…³é”®çš„æ­¥éª¤ï¼š

### 1ã€åˆå§‹åŒ–é›†ç¾¤

DDPçš„è®­ç»ƒä¸­ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦é€‰æ‹©ä¸€ä¸ªåˆ†å¸ƒå¼é€šä¿¡çš„åç«¯è¿›è¡Œåˆå§‹åŒ–ã€‚ä¸»è¦çš„å››ä¸ªåç«¯æˆ‘ä»¬[ä¹‹å‰çš„å›¾](#_5)ä¹Ÿå±•ç¤ºè¿‡äº†ã€‚åç«¯é€‰æ‹©çš„åŸºæœ¬åŸåˆ™å¯ä»¥çœ‹[Which backend to use?](https://docs.pytorch.org/docs/stable/distributed.html#which-backend-to-use)ã€‚

- glooï¼šCPUè®­ç»ƒæ¨èçš„åç«¯
- mpi
- ncclï¼šGPUè®­ç»ƒæ¨èçš„åç«¯ï¼ˆä»…Nå¡æ”¯æŒï¼Œä¹Ÿæ˜¯ç›®å‰æœ€é«˜æ•ˆçš„åç«¯ï¼‰
- ucc
- ç­‰ç­‰

é€‰å¥½äº†åç«¯ä¹‹åï¼ˆé€šå¸¸è¦ä¹ˆæ˜¯glooè¦ä¹ˆæ˜¯ncclï¼‰ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆå§‹åŒ–é›†ç¾¤äº†ï¼š

```python
dist.init_process_group("nccl")
```

è¿™æ¡ä»£ç æ‰§è¡Œçš„æ—¶å€™ï¼Œtorchä¼šå°è¯•åœ¨é›†ç¾¤ä¹‹é—´ç›¸äº’é€šä¿¡ï¼ˆç”±äºä¸åŒè¿›ç¨‹å¯åŠ¨é¡ºåºçš„é—®é¢˜ï¼Œå¯èƒ½ä¼šå› æ­¤å‡ºç°ä¸€äº›warningï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼‰ã€‚

åˆå§‹åŒ–å®Œæˆä¹‹åï¼Œæ‰€æœ‰çš„è¿›ç¨‹éƒ½ä¼šå¾—åˆ°ä¸€ä¸ªå…¨å±€çš„ç¼–å·ï¼š

```python
rank = dist.get_rank()
```

ä¾‹å¦‚å‚æ•°ä¸º`--nnodes=2 --nproc_per_node=8`çš„æƒ…å†µä¸‹ï¼Œrankå°±æ˜¯`0-15`ã€‚ç”»ä¸ªå›¾å¸®åŠ©ä¸€ä¸‹ç†è§£ï¼š

```mermaid
graph LR
subgraph Node1
0;1;2;3;4;5;6;7
end
subgraph Node2
8;9;10;11;12;13;14;15
end
```

### 2ã€å‡†å¤‡æ¨¡å‹

æ¨¡å‹çš„å‡†å¤‡å°±ç®€å•å¤šäº†ï¼Œå’ŒDPå‡ ä¹ä¸€è‡´ï¼š

```python
local_rank = int(os.environ["LOCAL_RANK"]) # è¿›ç¨‹çš„æœ¬æœºrank
torch.cuda.set_device(local_rank)
model = ConvNet().cuda()
model = DistributedDataParallel(model, device_ids=[local_rank])
```

è¿™é‡Œéœ€è¦æ³¨æ„çš„ç‚¹æ˜¯`local_rank`è¿™ä¸ªå˜é‡æ˜¯ä»ç¯å¢ƒå˜é‡ä¸­è¯»å–çš„ã€‚

æˆ‘ä»¬å¯åŠ¨äº†åˆ†å¸ƒå¼è®­ç»ƒä¹‹åï¼Œä¸€å…±ä¼šå¼€å¯`nnodes * nproc_per_node`ä¸ªè¿›ç¨‹ï¼Œä¸€èˆ¬æƒ…å†µä¸‹æˆ‘ä»¬åªç»™ä¸€å¼ å¡åˆ†é…ä¸€ä¸ªè¿›ç¨‹ã€‚æ‰€ä»¥è¿™ä¸ªæ—¶å€™å¤§è‡´å¯ä»¥è®¤ä¸ºï¼š

```python
local_rank == rank % nproc_per_node
```

### 3ã€å‡†å¤‡æ•°æ®é›†

åœ¨DDPè®­ç»ƒä¸­ï¼Œæˆ‘ä»¬çš„DataLoaderéœ€è¦ä½¿ç”¨[DistributedSampler](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler)ï¼Œè¿™æ ·æ‰èƒ½**ç¡®ä¿ä¸åŒçš„è¿›ç¨‹æ¥æ”¶åˆ°çš„æ•°æ®æ˜¯äº’æ–¥çš„**ï¼Œä»è€Œå®ç°ä¸€ä¸ªmini-batchæ•°æ®çš„æœ‰æ•ˆåˆ‡åˆ†ã€‚è¿™ä¸€ç‚¹æˆ‘ä»¬åœ¨ä¹‹å‰çš„[æ•°æ®åŠ è½½ç¬”è®°](./data.md)ä¸­æåˆ°è¿‡ã€‚

æ­¤å¤–æœ‰ä¸€ä¸ªå°é™·é˜±æ˜¯ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªepochéƒ½åº”è¯¥æ‰‹åŠ¨callä¸€ä¸‹`DistributedSampler.set_epoch`å‡½æ•°ï¼Œå¦åˆ™æ¯ä¸ªepoché‡‡æ ·æ•°æ®çš„é¡ºåºä¸ä¼šå˜ï¼š

```python
train_sampler.set_epoch(epoch)
```

è¿™æ ·åšæ˜¯ä¸ºäº†**åœ¨ä¸åŒè¿›ç¨‹ä¹‹é—´ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­**ï¼Œä¿è¯è¿›ç¨‹é—´é‡‡æ ·çš„é¡ºåºä¸€è‡´ã€‚`DistributedSampler`çš„å¤§è‡´åŸç†å’Œå…¶ä»–çš„`Sampler`å·®ä¸å¤šï¼Œ==éƒ½æ˜¯é’ˆå¯¹map-styleçš„æ•°æ®é›†è®¾è®¡çš„==ã€‚`Sampler`ä¼šå…ˆè¯»å–æ•°æ®é›†çš„æ‰€æœ‰keysï¼Œç„¶å**æ— æ”¾å›éšæœºé‡‡æ ·**è‹¥å¹²ä¸ªkeyå½¢æˆä¸€ä¸ªmini-batchçš„æ•°æ®ã€‚åªä¸è¿‡`DistributedSampler`ä¼šæŠŠé‡‡æ ·å‡ºçš„keysè¿›ä¸€æ­¥åˆ’åˆ†åˆ°ä¸åŒçš„è¿›ç¨‹ã€‚

!!! warning "iterable-styleæ•°æ®é›†éœ€è¦æ‰‹åŠ¨åˆ’åˆ†æ•°æ®é›†"
    æˆ‘ä»¬çŸ¥é“torchçš„æ•°æ®é›†æœ‰ä¸¤ç§å½¢æ€ï¼š

    - map-style
    - iterable-style

    å¯¹äºmap-styleçš„æ•°æ®é›†ï¼Œä½¿ç”¨`DistributedSampler`æ˜¯æ ‡å‡†çš„åšæ³•ã€‚

    ç„¶è€Œå¯¹äºiterable-styleçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬åè€Œ**ä¸èƒ½ä½¿ç”¨DistributedSampler**ï¼Œè¯¦ç»†çš„åŸå› [ä¹‹å‰](./data.md#iterable-style_1)ä»‹ç»è¿‡ã€‚

    > ç›´è§‚ä¸Šä¹Ÿå¾ˆå¥½ç†è§£ï¼Œiterable-styleçš„æ•°æ®é›†ä»è®¾è®¡ä¸Šæ¥è¯´å°±æ˜¯æŒ‰ç…§æ—¢å®šçš„é¡ºåºæŒ¨ä¸ªéå†ï¼Œå‹æ ¹æ— æ³•ç¡®å®šæœ‰å¤šå°‘ä¸ªæ ·æœ¬ï¼ˆæ²¡æœ‰å®ç°`__len__`ï¼‰ï¼Œè‡ªç„¶å°±æ— æ³•è¿›è¡Œéšæœºé‡‡æ ·ã€‚

    è¿™ä¸ªæ—¶å€™ï¼Œä¸ºäº†é€‚é…åˆ†å¸ƒå¼è®­ç»ƒæˆ‘ä»¬éœ€è¦**æ‰‹åŠ¨æ§åˆ¶æ•°æ®é›†çš„åˆ’åˆ†**ã€‚
    
    æ­¤å¤–è¿˜éœ€è¦æ³¨æ„å’Œ`num_workers`è¿›è¡Œå…¼å®¹ï¼Œå› ä¸ºiterable-styleæ•°æ®é›†åœ¨å¯ç”¨**å¤šè¿›ç¨‹åŠ è½½**çš„æ—¶å€™ä¹Ÿéœ€è¦è¿›è¡Œåˆ‡åˆ†ã€‚
    
    æ­¤æ—¶å¯ä»¥ä½¿ç”¨`DataLoader`çš„`worker_init_fn`å‚æ•°æ§åˆ¶æ•°æ®åœ¨æ¯ä¸ªworkerçš„åŠ è½½è¡Œä¸ºã€‚è¿™ä¸ªæˆ‘ä»¬[ä¹‹å‰çš„ç¬”è®°](./data.md#worker_init_fn)ä¹Ÿä»‹ç»è¿‡ã€‚

    ä¸‹é¢ç»™å‡ºäº†ä¸€ä¸ªå®Œæ•´çš„ä¾‹å­ï¼Œå¸Œæœ›èƒ½å¸®ä½ ç†è§£ã€‚

??? example "ä¸€ä¸ªå®Œæ•´çš„ä¾‹å­ï¼šDPP+å¤šè¿›ç¨‹Dataloader+IterableDataset"
    åœ¨DPP+å¤šè¿›ç¨‹Dataloaderçš„åœºæ™¯ä¸‹ï¼Œiterable-styleæ•°æ®é›†çš„åŠ è½½éœ€è¦ç‰¹åˆ«å°å¿ƒå¤„ç†ã€‚

    å‡è®¾ç°åœ¨éœ€è¦åœ¨4ä¸ªèŠ‚ç‚¹ä¸Šè¿›è¡ŒDDPè®­ç»ƒï¼Œæ¯ä¸ªè¿›ç¨‹çš„Dataloaderè®¾ç½®`num_workers=3`ï¼Œæ‹“æ‰‘å…³ç³»å¦‚ä¸‹ï¼š
    
    ```mermaid
    graph LR
    DDP --- NodeA & NodeB & NodeC & NodeD
    NodeA --- DataLoaderWorker_A1 & DataLoaderWorker_A2 & DataLoaderWorker_A3
    ```

    æˆ‘ä»¬çš„æ•°æ®å­—åº”è¯¥è¾ƒä¸ºå‡åŒ€åœ°åˆ’åˆ†åˆ°è¿™äº›èŠ‚ç‚¹ã€‚**ç°å®æƒ…å†µä¸‹ï¼Œiterable-styleçš„æ•°æ®é›†æ²¡æœ‰å®šä¹‰`__len__`æ–¹æ³•**ï¼Œæ— æ³•è·å¾—å‡†ç¡®çš„å¤§å°ã€‚
    
    å› æ­¤æˆ‘ä»¬åªèƒ½åœ¨æ¯ä¸ªDatasetå†…éƒ¨ç»´æŠ¤ä¸€ä¸ªè®¡æ•°å™¨ã€‚ç„¶åé€šè¿‡æ¨¡`world_size`ä»¥åŠæ¨¡`num_workers`çš„æ–¹å¼æ¥åˆ†é…æ•°æ®ï¼ˆéœ€è¦é€‚å½“é€‰å–ä½¿å¾—å®ƒä»¬ä¿©äº’è´¨ï¼‰ï¼š

    ```python title="IterableDatasetåœ¨DPP+å¤šè¿›ç¨‹Dataloaderä¸­çš„å¤„ç†" hl_lines="21-22"
    from itertools import count
    from torch.utils.data import DataLoader, IterableDataset, get_worker_info
    import torch.distributed as dist


    class IkunDataSet(IterableDataset):
        def __init__(self, rank, world_size):
            self.rank = rank
            self.world_size = world_size
            self.worker_id = 0
            self.num_workers = 1
            self.count = 0
            # æ— é™æ­£æ•´æ•°æ•°æ®æµï¼Œæ¨¡æ‹ŸæŸç§åœ¨çº¿æ•°æ®æµ
            self.data = count()

        def __iter__(self):
            for single_data in self.data:
                # è¿™ä¸ªæ•°æ®é›†å·²ç»ç”Ÿäº§çš„æ•°æ®ä¸ªæ•°
                self.count += 1
                if (
                    self.count % self.world_size == self.rank
                    and self.count % self.num_workers == self.worker_id
                ):
                    yield f"From rank-{self.rank}-data-{single_data}"


    def worker_init_fn(worker_id):
        worker_info = get_worker_info()
        # æ¯ä¸ªworkerä¼šæ‹¿åˆ°æ•°æ®é›†çš„ä¸€ä¸ªå¤åˆ¶
        dataset = worker_info.dataset
        # æŠŠworkerçš„ä¿¡æ¯ä¼ é€’ç»™æ•°æ®é›†
        dataset.worker_id = worker_id
        dataset.num_workers = worker_info.num_workers


    if __name__ == "__main__":
        dist.init_process_group(backend="gloo")
        dataset = IkunDataSet(rank=dist.get_rank(), world_size=dist.get_world_size())
        # è¿™é‡Œçš„num_workerså¿…é¡»è¦å–å¾—å’Œworld_sizeäº’è´¨
        dataloader = iter(DataLoader(dataset, num_workers=3, worker_init_fn=worker_init_fn))
        # æˆ‘ä»¬è·å–10æ¡æ•°æ®ï¼Œç„¶åè¾“å‡ºåˆ°æŒ‡å®šçš„æ–‡ä»¶
        f = open(f"{dist.get_rank()}-data.txt", "w")
        for _ in range(10):
            data = next(dataloader)
            print(data, file=f, flush=True)
        f.close()
        dist.destroy_process_group()

    ```

    ä½ å¯ä»¥åœ¨åå°çœ‹åˆ°æˆ‘ä»¬åˆ›å»ºçš„è¿›ç¨‹ï¼š

    ![](assets/2025-08-08-00-27-49.png)

    æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å››ä¸ªè¿›ç¨‹è·å¾—çš„æ•°æ®å¦‚ä¸‹ï¼š

    ![](assets/2025-08-08-00-32-58.png)

    æ³¨æ„ï¼Œé¡ºåºè¢«æ‰“ä¹±äº†ï¼ˆå› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†å¤šä¸ªdataload workersï¼Œå®ƒä»¬è¾“å‡ºæ•°æ®çš„ä¸ä¿è¯é¡ºåºï¼‰ã€‚å¦‚æœä½ ä½¿ç”¨`num_workers=1`å°±å¯ä»¥è§‚å¯Ÿåˆ°é¡ºåºæœªè¢«æ‰“ä¹±ã€‚

### 4ã€å¯åŠ¨è®­ç»ƒ

å¤šèŠ‚ç‚¹å¯åŠ¨å’Œå•æœºå¤šå¡å¯åŠ¨ä¹‹å‰éƒ½ä»‹ç»è¿‡äº†ï¼š

```bash title="å¤šèŠ‚ç‚¹å¯åŠ¨"
torchrun --nnodes=2 --nproc_per_node=8 \
    --rdzv_id=100 --rdzv_backend=c10d \
    --rdzv_endpoint=$MASTER_ADDR:29400 mnist_ddp.py --epochs 10
```

```bash title="å•æœºå¤šå¡å¯åŠ¨"
torchrun --nproc_per_node=4 mnist_ddp.py --epochs 10
```

æ­¤è‡´ã€‚
